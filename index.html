<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"kennychen.frontiertechcap.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="AWS Sagemaker Blog">
<meta property="og:url" content="https://kennychen.frontiertechcap.com/index.html">
<meta property="og:site_name" content="AWS Sagemaker Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Kenny Chen">
<meta property="article:tag" content="AWS, Segemaker, Machine Learning, Trading Strategies">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://kennychen.frontiertechcap.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>AWS Sagemaker Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">AWS Sagemaker Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Machine Learning for Financial Services and Trading Strategies</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kennychen.frontiertechcap.com/2024/01/27/most-three-important-qa-of-trading-strategy-deployment-for-aws-sagemaker/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/icon/ic_avatar_001.jpg">
      <meta itemprop="name" content="Kenny Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AWS Sagemaker Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/01/27/most-three-important-qa-of-trading-strategy-deployment-for-aws-sagemaker/" class="post-title-link" itemprop="url">Most three important Q&A of Trading Strategy Deployment for AWS SageMaker</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-01-27 00:00:00" itemprop="dateCreated datePublished" datetime="2024-01-27T00:00:00+08:00">2024-01-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Sagemaker/" itemprop="url" rel="index"><span itemprop="name">Sagemaker</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Sagemaker/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>I am extremely delighted to have participated in the AWS <a target="_blank" rel="noopener" href="https://aws.amazon.com/events/reinvent-recap-hongkong/">re:Invent re:Cap</a> event held in Hong Kong, which provided me with exposure to the latest AI solutions offered by AWS.</p>
<p>In my previous article, although I discussed deploying deep learning models in production using EC2, such a solution is only suitable for my personal use case, which can be found in the article “<a href="/blog.aws/2023/12/05/machine-learning-trading-strategy-best-practices-for-aws-sagemaker/">Machine Learning Trading Strategy Best Practices for AWS SageMaker</a>“.</p>
<p>In this article, I will first discuss the advantages of deploying models in production using SageMaker after training them locally. I would like to express my gratitude to <a target="_blank" rel="noopener" href="https://www.linkedin.com/in/hatted/">Raymond Tsang</a> for providing valuable insights.</p>
<p>Next, I will delve into the benefits of training models using SageMaker as opposed to local training. I would like to thank <a target="_blank" rel="noopener" href="https://www.linkedin.com/in/yanwei-cui/">Yanwei CUI</a> for sharing their insights.</p>
<p>Lastly, I will explain a more efficient trading strategy architecture, with special thanks to <a target="_blank" rel="noopener" href="https://www.linkedin.com/in/wingso/">Wing So</a> for their valuable input.</p>
<h2 id="1-The-Benefits-of-Deploying-Models-in-Production-with-SageMaker"><a href="#1-The-Benefits-of-Deploying-Models-in-Production-with-SageMaker" class="headerlink" title="1. The Benefits of Deploying Models in Production with SageMaker"></a>1. The Benefits of Deploying Models in Production with SageMaker</h2><p>The greatest advantage of SageMaker lies in its <code>data security</code>, <code>auto scaling</code>, and <code>container deployment</code> capabilities. If high data security, handling sudden traffic spikes, and agile development processes are required, leveraging these advantages of SageMaker can significantly accelerate development and deployment timelines.</p>
<p>However, after training models locally, can one deploy them in production using SageMaker? In other words, is it possible to <u>utilize only specific functionalities</u> of SageMaker?</p>
<p>Answer: Yes, it is possible to use only certain functionalities of SageMaker.</p>
<p>In the case of my use case, “Alice’s Intraday Futures Trading Strategy,” which is a daily trading strategy model with fixed trading times and a predictable number of requests, the model is susceptible to market sentiment and unexpected news events, necessitating monthly model updates.</p>
<p>In such a scenario, deploying the model in a production environment using SageMaker offers the following advantages:</p>
<ul>
<li>SageMaker allows for <code>container deployment</code>, making it easier to manage custom inference code within the deployment image.</li>
<li>SageMaker’s endpoint supports <code>version iterations</code>, facilitating agile development processes.</li>
<li>SageMaker supports <code>multi-model</code> deployment in a <code>single endpoint</code>, enabling easier management of multiple model interfaces.</li>
</ul>
<p>While local model training is preferred in my use case, there are still advantages to using SageMaker for model training.</p>
<h2 id="2-The-Advantages-of-Training-Models-with-SageMaker"><a href="#2-The-Advantages-of-Training-Models-with-SageMaker" class="headerlink" title="2. The Advantages of Training Models with SageMaker"></a>2. The Advantages of Training Models with SageMaker</h2><p>If there are two RTX3080 graphics cards available on the local server, is there still a need to use AWS SageMaker for training models? In other words, can one replace the <code>pay-as-you-go</code> model training of SageMaker with a <code>one-time higher fixed cost</code>?</p>
<p>Answer: Yes, it is possible. However, if one wishes to avoid the time-consuming process of hardware deployment or simply desires to utilize higher-end hardware for a shorter duration, training models using SageMaker is more suitable.</p>
<p>Furthermore, SageMaker optimizes <code>data-batch processing</code> and <code>floating-point operations</code> to accelerate model training speed.</p>
<p>In the case of my use case, “Diana’s Medium-Term Quarterly Trading Strategy,” which involves multi-asset trading in <u>four major markets</u> (US stocks, Hong Kong stocks, US bonds, and USD currency), the optimized <code>data-batch processing</code> of SageMaker can be utilized for the four main markets.</p>
<p>Additionally, the optimized <code>floating-point operations</code> of SageMaker can be applied to the <u>three core technical indicators</u> within the model (high dividend stocks, low volatility, and capital accumulation).</p>
<p>Therefore, gaming graphics cards have limitations when it comes to model training.</p>
<h2 id="3-A-More-Efficient-Trading-Strategy-Architecture"><a href="#3-A-More-Efficient-Trading-Strategy-Architecture" class="headerlink" title="3. A More Efficient Trading Strategy Architecture"></a>3. A More Efficient Trading Strategy Architecture</h2><p>Whether using EC2 or SageMaker container deployment, both options serve to expedite development time. However, considering the overall efficiency of the trading system, two factors need to be considered: <code>streaming data processing</code> and <code>the layer at which computations</code> are performed.</p>
<p><img src="/images/most-three-important-qa-of-trading-strategy-deployment-for-aws-sagemaker/architecture.png" alt="Full Architecture"></p>
<p>The key to achieving higher efficiency lies in the <code>Queue layer</code>.</p>
<p>After the Data Provider delivers streaming data, the Queue distributes the data to the Application while simultaneously storing the streaming data in a database. This reduces latency and improves overall efficiency.</p>
<p>Furthermore, performing computations at the Queue layer for the technical indicators used by all Applications prevents redundant calculations and enhances overall efficiency.</p>
<p>However, further investigation is required to determine which Queue framework to use.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>The theme of AWS <u>re:Invent re:Cap</u>, “Gen AI,” was a captivating event. There were many intriguing segments, such as the “Deep Dive Lounge,” “Lighting Talk,” and “Game Jam,” which provided delightful surprises.</p>
<p><img src="/images/most-three-important-qa-of-trading-strategy-deployment-for-aws-sagemaker/deep-dive-lounge.jpg" alt="Deep Dive Lounge"><u>Deep Dive Lounge, Wing So.</u></p>
<p>More importantly, numerous AWS solution architects have contributed to the advancement of my trading endeavors, offering lower-cost solutions and improved computational efficiency. Lastly, I would like to express my special thanks to <u>Raymond Tsang</u>, <u>Yanwei CUI</u>, and <u>Wing So</u> for their invaluable assistance.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kennychen.frontiertechcap.com/2023/12/05/machine-learning-trading-strategy-best-practices-for-aws-sagemaker/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/icon/ic_avatar_001.jpg">
      <meta itemprop="name" content="Kenny Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AWS Sagemaker Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/12/05/machine-learning-trading-strategy-best-practices-for-aws-sagemaker/" class="post-title-link" itemprop="url">Machine Learning Trading Strategy Best Practices for AWS SageMaker</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-12-05 00:00:00" itemprop="dateCreated datePublished" datetime="2023-12-05T00:00:00+08:00">2023-12-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Sagemaker/" itemprop="url" rel="index"><span itemprop="name">Sagemaker</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Sagemaker/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In my previous articles, I used two different trading strategies to explain the best practices of <code>batch-transform</code> and <code>real-time endpoints</code>, as well as the reasons for using EC2. These articles can be referred to as “<a href="/blog.aws/2023/10/27/even-though-sagemaker-provides-various-benefits-why-do-i-still-use-ec2/">Even though Sagemaker provides various benefits, why do I still use EC2?</a>“ and “<a href="/blog.aws/2023/09/13/why-choose-sagemaker-despite-having-a-local-server-with-rtx3080/">Why Choose Sagemaker Despite Having a Local Server with RTX3080?</a>“.</p>
<p>In this article, I will first demonstrate the complete architecture of SageMaker.</p>
<p>Then, I will explain the reasons for using <code>Multi-Modal-Single-Container</code> + <code>Microservices</code> and not using <code>Application Load Balancer</code>.</p>
<p>Finally, I will use two different trading strategies to explain the best practices of <code>data parallelism</code> and <code>model parallelism</code> in advanced training models.</p>
<h2 id="Architecture-Overview"><a href="#Architecture-Overview" class="headerlink" title="Architecture Overview"></a>Architecture Overview</h2><p><img src="/images/machine-learning-trading-strategy-best-practices-for-aws-sagemaker/architecture-overview.png" alt="Architecture Overview"></p>
<p><strong>Local Development Environment</strong></p>
<ul>
<li><code>CUDA 11.5</code> and <code>Nvidia-container-toolkit</code> for local model training.</li>
<li><code>jupyter/tensorflow-notebook</code> for local development environment, with libraries required for <code>Sagemaker[local]</code>, <code>Backtrader</code>, and <code>Monitor Web UI</code> installed in the image.</li>
</ul>
<p><strong>Supported AWS services</strong></p>
<ul>
<li><code>Sagemaker prebuilt images</code> for pulling images to the <code>local development environment</code> for local model training and testing.</li>
<li><code>S3 Bucket</code> for storing datasets and models.</li>
<li><code>CodePipline </code>for deploying projects on <code>Github</code> to <code>EC2</code> production environment.</li>
</ul>
<p><strong>EC2</strong></p>
<ul>
<li><code>Custom Production Container</code> with libraries required for Sagemaker, Backtrader, and Monitor Web UI.</li>
<li><code>Monitor Web UI</code> for presenting the trading performance of the model in graphical form, providing <code>:80</code> to Trader and Asset Portfolio Manager.</li>
<li><code>Server Image</code> for deploying models using Sagemaker prebuilt image, providing <code>:8080</code> to business user.</li>
</ul>
<p><strong>Managed AWS Services</strong></p>
<ul>
<li><code>RDS</code> for storing model results. Monitor Web UI in EC2 retrieves the data from RDS and presents the trading performance in graphical form.</li>
<li><code>CloudWatch</code> for monitoring the computation and storage of EC2, RDS, and S3 Bucket.</li>
<li><code>IAM</code> for helping jupyter&#x2F;tensorflow-notebook in local development environment to access Sagemaker prebuilt images and S3 Bucket.</li>
</ul>
<h2 id="Why-not-use-Application-Load-Balancer-and-instead-create-Multi-Modal-Single-Container-Microservices-on-EC2-to-handle-errors"><a href="#Why-not-use-Application-Load-Balancer-and-instead-create-Multi-Modal-Single-Container-Microservices-on-EC2-to-handle-errors" class="headerlink" title="Why not use Application Load Balancer and instead create Multi-Modal-Single-Container + Microservices on EC2 to handle errors?"></a>Why not use <code>Application Load Balancer</code> and instead create <code>Multi-Modal-Single-Container</code> + <code>Microservices</code> on EC2 to handle errors?</h2><p><img src="/images/machine-learning-trading-strategy-best-practices-for-aws-sagemaker/application-load-balancer.png" alt="Application Load Balancer"></p>
<p><code>Application Load Balancer</code> is a remarkable service. In fact, it can also be used to handle errors. However, in the case of trading strategies, I would choose to handle errors with <code>Multi-Modal-Single-Container</code> + <code>Microservices</code>. </p>
<p>Here are my three error handling methods:</p>
<p><img src="/images/machine-learning-trading-strategy-best-practices-for-aws-sagemaker/multi-modal-single-container.png" alt="three error handling methods"></p>
<p>The goal of the following three error handling methods is to flexibly reduce hardware resource requirements.</p>
<p><strong>1.Switch to Smallest Model</strong></p>
<p>There are two trading strategies (Diana’s medium-term quarterly trading strategy and Alice’s intraday futures trading strategy). Each trading strategy has two versions of the model, where the <code>Biggest Model</code> provides high accuracy but requires high hardware resources. On the contrary, the <code>Smallest Model</code> provides low accuracy but requires low hardware resources.</p>
<p>If the server is in a high computational state, switching to the <code>Smallest Model</code> can reduce the hardware resource requirements and keep the application running smoothly.</p>
<p><strong>2. Response caching results</strong></p>
<p>When the same business user uses the application frequently, returning cached data can avoid overloading hardware resources.</p>
<p><strong>3. Delayed Response time</strong></p>
<p>When hardware resources are overloaded, delaying the response time can release the hardware resources.</p>
<h2 id="Advantages-of-Multi-Modal-Single-Container-Microservices"><a href="#Advantages-of-Multi-Modal-Single-Container-Microservices" class="headerlink" title="Advantages of Multi-Modal-Single-Container + Microservices"></a>Advantages of <code>Multi-Modal-Single-Container</code> + <code>Microservices</code></h2><p>Here are my examples of trading strategies to explain the reasons for using <code>Multi-Modal-Single-Container</code> + <code>Microservices</code>.</p>
<p><strong>1.Trading strategies have high fault tolerance</strong></p>
<p>Both trading strategies anticipate reduced profits due to slippage during trading. This design with high fault tolerance can accommodate various hardware issues, such as switching to the Smallest Model, response caching results, and delayed response time.</p>
<p>Additionally, it can handle errors from market makers, such as delayed quotes, partial executions, and wide bid-ask spreads.</p>
<p><strong>2. Shared hardware resources</strong></p>
<p>The frequency and time of use of two trading strategies are different, allowing for full utilization of idle hardware resources.</p>
<p><strong>3. Deployment of trading strategies in different regions</strong></p>
<p>Diana’s medium-term quarterly trading strategy targets global assets. By deploying trading strategies independently in Hong Kong and the United States, the latency can be reduced.</p>
<p>Furthermore, if the hardware in Hong Kong completely stops working, the hardware in the United States can be used to hedge the risk by purchasing short options of overseas ETF.</p>
<h2 id="Best-Practices-of-Data-Parallelism-and-Model-Parallelism-in-Advanced-Training-Models"><a href="#Best-Practices-of-Data-Parallelism-and-Model-Parallelism-in-Advanced-Training-Models" class="headerlink" title="Best Practices of Data Parallelism and Model Parallelism in Advanced Training Models"></a>Best Practices of Data Parallelism and Model Parallelism in Advanced Training Models</h2><p>Sagemaker provides remarkable advanced training methods: <code>Data parallelism</code> and <code>Model parallelism</code>. I will use two different trading strategies to explain the best practices of data parallelism and model parallelism in advanced training models.</p>
<p><strong>Data parallelism</strong><br><img src="/images/machine-learning-trading-strategy-best-practices-for-aws-sagemaker/data-parallelism.png" alt="Data parallelism"></p>
<p><strong>Model parallelism</strong><br><img src="/images/machine-learning-trading-strategy-best-practices-for-aws-sagemaker/model-parallelism.png" alt="Model parallelism"></p>
<blockquote>
<ul>
<li><code>Model Parallelism</code>: A simple method of model parallelism is to explicitly assign layers of the model onto different devices.</li>
<li><code>Data Parallelism</code>: Each individual training process has a copy of the global model but trains it on a unique slice of data in parallel with others.</li>
</ul>
</blockquote>
<blockquote>
<p>– Accelerate Deep Learning Workloads with Amazon SageMaker, chapter10</p>
</blockquote>
<p>In simple terms, if the data can be divided into small groups, <code>Data parallelism</code> is used. If the model can be divided into small groups, <code>Model parallelism</code> is used.</p>
<p><strong>Alice’s intraday futures trading strategy</strong></p>
<p><img src="/images/machine-learning-trading-strategy-best-practices-for-aws-sagemaker/alice-intraday-futures-trading-strategy-001.png" alt="Alice&#39;s intraday futures trading strategy"></p>
<p>The intraday trading strategy mainly uses a few key indicators to train the model, providing entry and exit points. Therefore, the data samples are large.</p>
<p><img src="/images/machine-learning-trading-strategy-best-practices-for-aws-sagemaker/alice-intraday-futures-trading-strategy-002.png" alt="Alice&#39;s intraday futures trading strategy"></p>
<p>When the data sample is large and the model has only a few algorithms, <code>Data parallelism</code> should be used to train the model. This allows the data set to be split and computed on different GPUs.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">distribution = &#123; </span><br><span class="line">    &quot;smdistributed&quot;: &#123; </span><br><span class="line">        &quot;dataparallel&quot;: &#123;</span><br><span class="line">            &quot;enabled&quot;: True, </span><br><span class="line">            &quot;custom_mpi_options&quot;: &quot;-verbose -x NCCL_DEBUG=VERSION&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/Chapter06/3_SDP_finetuning_pytorch_models.ipynb">3_SDP_finetuning_pytorch_models.ipynb</a></p>
<p>Sagemaker provides remarkable advanced training methods. By setting the distribution parameter, Data parallelism can be used to train the model.</p>
<p><strong>Diana’s Medium-Term Quarterly Trading Strategy</strong></p>
<p><img src="/images/machine-learning-trading-strategy-best-practices-for-aws-sagemaker/diana-medium-term-quarterly-trading-strategy-001.png" alt="Diana&#39;s Medium-Term Quarterly Trading Strategy"></p>
<p>The macro trading strategy mainly uses dozens of key indicators to provide overseas asset allocation forecasts. The minimum data set is 8 years (2 bull and bear cycles) of hourly snapshot data.</p>
<p><img src="/images/machine-learning-trading-strategy-best-practices-for-aws-sagemaker/diana-medium-term-quarterly-trading-strategy-002.png" alt="Diana&#39;s Medium-Term Quarterly Trading Strategy"></p>
<p>When the main algorithms can be split into small groups, <code>Model parallelism</code> is used to train the model. This allows the model tensor to be computed in batches on different GPUs.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">distribution=&#123;</span><br><span class="line">    &quot;smdistributed&quot;: &#123;</span><br><span class="line">        &quot;modelparallel&quot;: &#123;</span><br><span class="line">            &quot;enabled&quot;:True,</span><br><span class="line">            &quot;parameters&quot;: &#123;</span><br><span class="line">                &quot;microbatches&quot;: 8,</span><br><span class="line">                &quot;placement_strategy&quot;: &quot;cluster&quot;,</span><br><span class="line">                &quot;pipeline&quot;: &quot;interleaved&quot;,</span><br><span class="line">                &quot;optimize&quot;: &quot;speed&quot;, </span><br><span class="line">                &quot;partitions&quot;: 2,</span><br><span class="line">                &quot;auto_partition&quot;: True,</span><br><span class="line">                &quot;ddp&quot;: True,</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;mpi&quot;: &#123;</span><br><span class="line">          &quot;enabled&quot;: True,</span><br><span class="line">          &quot;processes_per_host&quot;: 1,</span><br><span class="line">          &quot;custom_mpi_options&quot;: &quot;-verbose -x orte_base_help_aggregate=0&quot; </span><br><span class="line">    &#125;,</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/Chapter06/4_SMP_finetuning_pytorch_models.ipynb">3_SDP_finetuning_pytorch_models.ipynb</a></p>
<p>Similarly, by setting the distribution parameter, Model parallelism can be used to train the model.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>AWS provides convenient solutions for the financial industry. Sagemaker seamlessly integrates deep learning workflow into production environments. Additionally, Sagemaker offers surprising features to accelerate development. I will continue to learn about new AWS products and share examples of AWS services in finance and trading.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kennychen.frontiertechcap.com/2023/10/27/even-though-sagemaker-provides-various-benefits-why-do-i-still-use-ec2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/icon/ic_avatar_001.jpg">
      <meta itemprop="name" content="Kenny Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AWS Sagemaker Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/10/27/even-though-sagemaker-provides-various-benefits-why-do-i-still-use-ec2/" class="post-title-link" itemprop="url">Even though Sagemaker provides various benefits, why do I still use EC2?</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-10-27 00:00:00" itemprop="dateCreated datePublished" datetime="2023-10-27T00:00:00+08:00">2023-10-27</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Sagemaker/" itemprop="url" rel="index"><span itemprop="name">Sagemaker</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Sagemaker/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In the previous article, I explained the benefits of using Sagemaker for training models on a local server, which can be found in the article “<a href="/blog.aws/2023/09/13/why-choose-sagemaker-despite-having-a-local-server-with-rtx3080/">Why Choose Sagemaker Despite Having a Local Server with RTX3080?</a>“.</p>
<p>In this article, I will first present a simple example to demonstrate the process of training and deploying models locally using Sagemaker.</p>
<p>Then, I will share my experience with a LSTM futures trading project to explain the best practices for using real-time endpoints and batch-transform endpoints.</p>
<p>Finally, based on my experience with the LSTM futures trading project, I will explain which Sagemaker Instance &#x2F; Fargate &#x2F; EC2 should be selected for deployment.</p>
<h2 id="Sagemaker-Exec-Training-and-Deploying-Models-Locally"><a href="#Sagemaker-Exec-Training-and-Deploying-Models-Locally" class="headerlink" title="Sagemaker Exec - Training and Deploying Models Locally"></a>Sagemaker Exec - Training and Deploying Models Locally</h2><p><img src="/images/even-though-sagemaker-provides-various-benefits-why-do-i-still-use-ec2/training-and-deploying-models-locally.png" alt="Sagemaker Exec - Training and Deploying Models Locally"></p>
<p><strong>0.0 Prerequisite:</strong><br>Before starting local development, please install the following:</p>
<ul>
<li>Nvidia CUDA (<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-downloads">https://developer.nvidia.com/cuda-downloads</a>)</li>
<li>Nvidia-container-toolkit (<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/nvidia-container-toolkit">https://github.com/NVIDIA/nvidia-container-toolkit</a>)</li>
<li>Docker (<a target="_blank" rel="noopener" href="https://docs.docker.com/engine/install/">https://docs.docker.com/engine/install/</a>)</li>
</ul>
<p><strong>1.0 Install Docker Local Development Image</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"># Copyright (c) Jupyter Development Team.</span><br><span class="line"># Distributed under the terms of the Modified BSD License.</span><br><span class="line">ARG REGISTRY=quay.io</span><br><span class="line">ARG OWNER=jupyter</span><br><span class="line">ARG BASE_CONTAINER=$REGISTRY/$OWNER/scipy-notebook</span><br><span class="line">FROM $BASE_CONTAINER</span><br><span class="line"></span><br><span class="line">USER root</span><br><span class="line"></span><br><span class="line">LABEL maintainer=&quot;Jupyter Project &lt;jupyter@googlegroups.com&gt;&quot;</span><br><span class="line"></span><br><span class="line">RUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \</span><br><span class="line">    ca-certificates \</span><br><span class="line">    curl  \</span><br><span class="line">    gnupg</span><br><span class="line">RUN install  -m 0755 -d /etc/apt/keyrings</span><br><span class="line">RUN curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg</span><br><span class="line">RUN chmod a+r /etc/apt/keyrings/docker.gpg</span><br><span class="line">RUN echo \</span><br><span class="line">  &quot;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \</span><br><span class="line">  $(. /etc/os-release &amp;&amp; echo &quot;$VERSION_CODENAME&quot;) stable&quot; | \</span><br><span class="line">  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null</span><br><span class="line">RUN apt-get update</span><br><span class="line">RUN apt-get install -y \</span><br><span class="line">    docker-ce \</span><br><span class="line">    docker-ce-cli \</span><br><span class="line">    containerd.io \</span><br><span class="line">    docker-buildx-plugin \</span><br><span class="line">    docker-compose-plugin</span><br><span class="line"></span><br><span class="line"># Fix: https://github.com/hadolint/hadolint/wiki/DL4006</span><br><span class="line"># Fix: https://github.com/koalaman/shellcheck/wiki/SC3014</span><br><span class="line">SHELL [&quot;/bin/bash&quot;, &quot;-o&quot;, &quot;pipefail&quot;, &quot;-c&quot;]</span><br><span class="line"></span><br><span class="line"># Install Tensorflow with pip</span><br><span class="line">RUN pip install --no-cache-dir tensorflow[and-cuda] &amp;&amp; \</span><br><span class="line">    fix-permissions &quot;$&#123;CONDA_DIR&#125;&quot; &amp;&amp; \</span><br><span class="line">    fix-permissions &quot;/home/$&#123;NB_USER&#125;&quot;</span><br><span class="line"></span><br><span class="line"># Install sagemaker-python-sdk with pip</span><br><span class="line">RUN pip install --no-cache-dir &#x27;sagemaker[local]&#x27; --upgrade</span><br></pre></td></tr></table></figure>
<p>1.1 Use the <code>jupyter/tensorflow-notebook</code> development environment<br>(<a target="_blank" rel="noopener" href="https://github.com/jupyter/docker-stacks/blob/main/images/tensorflow-notebook/Dockerfile">https://github.com/jupyter/docker-stacks/blob/main/images/tensorflow-notebook/Dockerfile</a>)<br>1.2 Modify the <code>jupyter/tensorflow-notebook</code> image to install <code>docker</code> and <code>sagemaker[local]</code> inside the image</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t sagemaker/local:0.1 .</span><br></pre></td></tr></table></figure>
<p>1.3 Create the local development image</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run --privileged --name jupyter.sagemaker.001 --gpus all -e GRANT_SUDO=yes --user root --network host -it -v /home/jovyan/work:/home/jovyan/work -v /sagemaker:/sagemaker -v /var/run/docker.sock:/var/run/docker.sock -v /tmp:/tmp -v /sagemaker:/sagemaker sagemaker/local:0.2 &gt;&gt; /home/jovyan/work/log/sagemaker_local_date +\%Y\%m\%d_\%H\%M\%S.log 2</span><br></pre></td></tr></table></figure>
<p>1.4 Start the local development image<br>1.5 <code>-v /home/jovyan/work</code>, this is the default path for jupyter&#x2F;tensorflow-notebook<br>1.6 <code>-v /var/run/docker.sock</code>, used to start the Sagemaker’s train &amp; inference image<br>1.7 <code>-v /tmp</code>, this is the temporary file path for Sagemaker<br>1.8 Go to <code>127.0.0.1:8888</code></p>
<p><strong>2.0 Sagemaker Local Training of Models</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">os.environ[&#x27;AWS_DEFAULT_REGION&#x27;] = &#x27;AWS_DEFAULT_REGION&#x27;</span><br><span class="line">os.environ[&#x27;AWS_ACCESS_KEY_ID&#x27;] = &#x27;AWS_ACCESS_KEY_ID&#x27;</span><br><span class="line">os.environ[&#x27;AWS_SECRET_ACCESS_KEY&#x27;] = &#x27;AWS_SECRET_ACCESS_KEY&#x27;</span><br><span class="line">os.environ[&#x27;AWS_ROLE&#x27;] = &#x27;AWS_ROLE&#x27;</span><br><span class="line">os.environ[&#x27;INSTANCE_TYPE&#x27;] = &#x27;local_gpu&#x27;</span><br></pre></td></tr></table></figure>
<p>2.1 Set AWS <code>IAM</code> and <code>INSTANCE_TYPE</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import keras</span><br><span class="line">import numpy as np</span><br><span class="line">from keras.datasets import fashion_mnist</span><br><span class="line"></span><br><span class="line">(x_train, y_train), (x_val, y_val) = fashion_mnist.load_data()</span><br><span class="line">os.makedirs(&quot;./data&quot;, exist_ok = True)</span><br><span class="line">np.savez(&#x27;./data/training&#x27;, image=x_train, label=y_train)</span><br><span class="line">np.savez(&#x27;./data/validation&#x27;, image=x_val, label=y_val)</span><br></pre></td></tr></table></figure>
<p>2.2 Download datasets (training set and validation set)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from sagemaker.tensorflow import TensorFlow</span><br><span class="line"></span><br><span class="line">training = &#x27;file://data&#x27;</span><br><span class="line">validation = &#x27;file://data&#x27;</span><br><span class="line">output = &#x27;file:///tmp&#x27;</span><br><span class="line"></span><br><span class="line">tf_estimator = TensorFlow(entry_point=&#x27;fmnist.py&#x27;,</span><br><span class="line">                          source_dir=&#x27;./src&#x27;,</span><br><span class="line">                          role=os.environ[&#x27;AWS_ROLE&#x27;],</span><br><span class="line">                          instance_count=1, </span><br><span class="line">                          instance_type=os.environ[&#x27;INSTANCE_TYPE&#x27;],</span><br><span class="line">                          framework_version=&#x27;2.11&#x27;, </span><br><span class="line">                          py_version=&#x27;py39&#x27;,</span><br><span class="line">                          hyperparameters=&#123;&#x27;epochs&#x27;: 10&#125;,</span><br><span class="line">                          output_path=output,</span><br><span class="line">                         )</span><br><span class="line"></span><br><span class="line">tf_estimator.fit(&#123;&#x27;training&#x27;: training, &#x27;validation&#x27;: validation&#125;)</span><br></pre></td></tr></table></figure>
<p>2.3 Download <code>fmnist.py</code> and <code>model.py</code> to <code>./src</code><br>(<a target="_blank" rel="noopener" href="https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition/tree/main/Chapter%2007/tf">https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition/tree/main/Chapter%2007/tf</a>)<br>2.4 Start local training of models. Sagemaker launches the image <code>763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.11-gpu-py39</code>.</p>
<p><strong>3.0 Sagemaker Local Deployment of Models</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">from sagemaker.tensorflow import TensorFlowModel</span><br><span class="line"></span><br><span class="line">model = TensorFlowModel(</span><br><span class="line">    entry_point=&#x27;inference.py&#x27;,</span><br><span class="line">    source_dir=&#x27;./src&#x27;,</span><br><span class="line">    role=os.environ[&#x27;AWS_ROLE&#x27;],</span><br><span class="line">    model_data=f&#x27;&#123;output&#125;/model.tar.gz&#x27;,</span><br><span class="line">    framework_version=&#x27;2.11&#x27;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">predictor = model.deploy(</span><br><span class="line">    initial_instance_count=1,</span><br><span class="line">    instance_type=os.environ[&#x27;INSTANCE_TYPE&#x27;],</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>3.1 Download <code>inference.py</code> to <code>./src</code><br>(<a target="_blank" rel="noopener" href="https://github.com/aws/sagemaker-tensorflow-serving-container/blob/master/test/resources/examples/test1/inference.py">https://github.com/aws/sagemaker-tensorflow-serving-container/blob/master/test/resources/examples/test1/inference.py</a>)<br>3.2 Create the Tensorflow-serving image. Sagemaker launches the image <code>763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.11-gpu</code></p>
<p><strong>4.0 Invoke the Tensorflow-Serving:8080 interface</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line">import json</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">num_samples = 10</span><br><span class="line">indices = random.sample(range(x_val.shape[0] - 1), num_samples)</span><br><span class="line">images = x_val[indices]/255</span><br><span class="line">labels = y_val[indices]</span><br><span class="line"></span><br><span class="line">for i in range(num_samples):</span><br><span class="line">    plt.subplot(1,num_samples,i+1)</span><br><span class="line">    plt.imshow(images[i].reshape(28, 28), cmap=&#x27;gray&#x27;)</span><br><span class="line">    plt.title(labels[i])</span><br><span class="line">    plt.axis(&#x27;off&#x27;)</span><br><span class="line"></span><br><span class="line">payload = images.reshape(num_samples, 28, 28, 1)</span><br></pre></td></tr></table></figure>
<p><img src="/images/even-though-sagemaker-provides-various-benefits-why-do-i-still-use-ec2/dataset-001.jpg" alt="Download datasets"><br>4.1 Download datasets</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">response = predictor.predict(payload)</span><br><span class="line">prediction = np.array(response[&#x27;predictions&#x27;])</span><br><span class="line">predicted_label = prediction.argmax(axis=1)</span><br><span class="line">print(&#x27;Predicted labels are: &#123;&#125;&#x27;.format(predicted_label))</span><br></pre></td></tr></table></figure>
<p>4.2 Run the model</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(&#x27;About to delete the endpoint&#x27;)</span><br><span class="line">predictor.delete_endpoint(predictor.endpoint_name)</span><br></pre></td></tr></table></figure>
<p>4.3 Close the Tensorflow-serving image</p>
<p><strong>5.0 External Invocation of Tensorflow-serving:8080 interface</strong></p>
<p><img src="/images/even-though-sagemaker-provides-various-benefits-why-do-i-still-use-ec2/postman-001.png" alt="External Invocation of Tensorflow-serving:8080 interface"><br>5.1 Go to the real-time endpoint (<a target="_blank" rel="noopener" href="http://your-segamaker-domain:8080/invocations">http://YOUR-SEGAMAKER-DOMAIN:8080/invocations</a>)<br>5.2 [Post] Body -&gt; raw, input json data</p>
<p><strong>Conclusion of Sagemaker Exec</strong></p>
<p>This is a simple example demonstrating the process of training and deploying models locally using Sagemaker. As mentioned earlier, since Sagemaker does not fully support local development, it is necessary to modify the <code>jupyter/tensorflow-notebook</code> image. Additionally, a more complex <code>inference.py</code> is required for local model deployment.</p>
<p>However, I still recommend using Sagemaker for local development because it provides pre-built resources and clean code. Moreover, Sagemaker has preconfigured workflows for training and deploying model images, so we do not need to deeply understand the project structure and internal operations to complete the training and deployment of models.</p>
<h2 id="When-to-use-real-time-endpoints-and-batch-transform-endpoints"><a href="#When-to-use-real-time-endpoints-and-batch-transform-endpoints" class="headerlink" title="When to use real-time endpoints and batch-transform endpoints"></a>When to use real-time endpoints and batch-transform endpoints</h2><p>The choice of endpoint depends not only on cost factors but also on business logic, such as response time, frequency of Invocation, dataset size, model update frequency, error tolerance, etc. I will present two practical use cases to explain the best use of <code>real-time endpoints</code> and <code>batch-transform endpoints</code>.</p>
<blockquote>
<ul>
<li>SageMaker batch transform is designed to perform batch inference at scale and is cost-effective.</li>
<li>SageMaker real-time endpoints aim to provide a robust live hosting option for your ML use cases.</li>
</ul>
</blockquote>
<blockquote>
<p>Getting-Started-with-Amazon-SageMaker-Studio, chapter07</p>
</blockquote>
<p>Here are two examples of trading strategy:</p>
<p><strong>1. Diana’s medium-term quarterly trading strategy</strong><br>The multi-asset portfolio includes US stocks, overseas stocks, US coupon bonds, overseas high-yield bonds, and 3-month bills. Every 3 months, the LSTM-all-weather-portfolio model is used for asset rebalancing. This model runs once a day, 15 minutes before market close, to check the risk of each position and whether the portfolio meets the 5% annualized return.</p>
<p><strong>2. Alice’s intraday futures trading strategy</strong><br>Trading only S&amp;P 500 index and Nasdaq index futures, with a holding period of approximately 30 minutes to 360 minutes. The LSTM-Pure-Alpha-Future model uses 20-second snapshot data to provide buy and exit signals. These signals are stored for daily performance analysis of the model.</p>
<hr>
<p><strong>Diana’s Medium-Term Quarterly Trading Strategy</strong></p>
<ul>
<li><strong>Assets</strong>: Stocks, Bonds, Bills</li>
<li><strong>Instrument Pool</strong>: US stocks, Overseas stocks, US coupon bonds, Overseas high-yield bonds, 3-month bills</li>
<li><strong>Trading Frequency</strong>: 5 trades per quarter </li>
<li><strong>Response Time</strong>: Time Delayed. Only required 15 minutes before market close</li>
<li><strong>Model</strong>: LSTM-all-weather-portfolio</li>
<li><strong>Model Update Frequency</strong>: Low. Update the model only if it achieves a 5% annualized return</li>
<li><strong>Recommended Solution</strong>: <code>Batch-transform endpoint</code></li>
</ul>
<p><img src="/images/even-though-sagemaker-provides-various-benefits-why-do-i-still-use-ec2/batch-transform-endpoint.png" alt="Batch-transform endpoint"></p>
<p>If the dataset is large and response time can be delayed, the <code>Batch-transform endpoint</code> should be used.</p>
<p><strong>Alice’s Intraday Futures Trading Strategy</strong></p>
<ul>
<li><strong>Assets</strong>: Index Futures</li>
<li><strong>Instrument Pool</strong>: SP500 index Future, Nasdaq Index Future</li>
<li><strong>Trading Frequency</strong>: 5 trades per day</li>
<li><strong>Response Time</strong>: Real-time</li>
<li><strong>Model</strong>: LSTM-Pure-Alpha-Future</li>
<li><strong>Model Update Frequency</strong>: High. Always optimization of buy and exit signals</li>
<li><strong>Recommended Solution</strong>: <code>Real-time endpoint</code></li>
</ul>
<p><img src="/images/even-though-sagemaker-provides-various-benefits-why-do-i-still-use-ec2/real-time-endpoint.png" alt="Real-time endpoint"></p>
<p>If the dataset is small and response time needs to be fast, the <code>Real-time endpoint</code> should be used.</p>
<hr>
<h2 id="Even-though-Sagemaker-provides-various-deployment-benefits-why-do-I-still-use-EC2"><a href="#Even-though-Sagemaker-provides-various-deployment-benefits-why-do-I-still-use-EC2" class="headerlink" title="Even though Sagemaker provides various deployment benefits, why do I still use EC2?"></a>Even though Sagemaker provides various deployment benefits, why do I still use EC2?</h2><p>In my current role at a financial technology company, I am always excited about innovative products. AWS’s innovative products bring surprising solutions. If I were to create a personal music brand, I would choose AWS’s new products such as DeepComposer, Fargate, Amplify, Lambda, etc.</p>
<p>However, the cost of migrating to the cloud is high. Additionally, there is no significant incentive to migrate existing hardware resources to the cloud. Here are my use cases to explain why I choose EC2:</p>
<p><img src="/images/even-though-sagemaker-provides-various-benefits-why-do-i-still-use-ec2/benefits-001.png" alt="Even though Sagemaker provides various deployment benefits, why do I still use EC2?"></p>
<p><strong>1. Custom Python financial engineering library</strong></p>
<p>Although I prefer to use frameworks and libraries, there are some special requirements that require the use of a custom Python financial engineering library, such as developing high dividend investment strategies, macro cross-market analysis, and so on. Therefore, I manage Docker images. Thus, the pre-built images provided by Sagemaker cannot fully meet my needs, and instead, EC2 offers more freedom to structure the production environment.</p>
<p><strong>2. Team development and custom CI&#x2F;CD workflow</strong></p>
<p>Although Sagemaker allows for quick training and deployment of models, it does not fully meet my development needs. We have an independent development team responsible for researching trading strategies and developing deep learning trading models. Due to our custom CI&#x2F;CD workflow, it is not suitable to overly rely on Sagemaker for architecture.</p>
<p><strong>3. Pursuit of controlled fixed costs</strong></p>
<p>Although Sagemaker and Fargate allow for quick creation of instances, the cost is based on CPU utilization. Therefore, I prefer EC2 with fixed costs and manually scale up when resources are insufficient.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Sagemaker is a remarkable product. For startup companies looking to launch new products, AWS’s cloud solution is the preferred choice. Even for mature enterprises, leveraging AWS cloud services can optimize workflow. In summary, I highly recommend incorporating Sagemaker into the development process.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kennychen.frontiertechcap.com/2023/09/13/why-choose-sagemaker-despite-having-a-local-server-with-rtx3080/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/icon/ic_avatar_001.jpg">
      <meta itemprop="name" content="Kenny Chen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AWS Sagemaker Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/09/13/why-choose-sagemaker-despite-having-a-local-server-with-rtx3080/" class="post-title-link" itemprop="url">Why Choose Sagemaker Despite Having a Local Server with RTX3080?</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-09-13 00:00:00" itemprop="dateCreated datePublished" datetime="2023-09-13T00:00:00+08:00">2023-09-13</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Sagemaker/" itemprop="url" rel="index"><span itemprop="name">Sagemaker</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Sagemaker/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>If I have a local server with an RTX3080 and 64GB of memory, do I still need AWS Sagemaker? The answer is: yes, there is still a need.</p>
<p><img src="/images/why-choose-sagemaker-despite-having-a-local-server-with-rtx3080/benefits-and-drawbacks.png" alt="benefits and drawbacks"></p>
<p>Although the hardware level of the local server is good, Sagemaker provides additional benefits that are particularly suitable for team development processes. These <strong><em>benefits</em></strong> include:</p>
<ol>
<li><p>Sagemaker automatically uploads datasets (training set, validation set) to S3 buckets, with a timestamp suffix each time a model is trained. This makes it easy to manage data sources during a long-term development process.</p>
</li>
<li><p>Sagemaker integrates several popular deep learning frameworks, such as TensorFlow and XGBoost. This ensures code consistency.</p>
</li>
<li><p>Sagemaker provides pre-built docker images for various deep learning frameworks, including training images and server images, which accelerate local development time.</p>
</li>
<li><p>The inference.py in Sagemaker’s server image ensures a unified interface specification for models. Code consistency and simplicity are crucial in team development.</p>
</li>
<li><p>Sagemaker itself is a cloud service, making it convenient to deploy deep learning model applications.</p>
</li>
</ol>
<p>However, Sagemaker has some drawbacks when it comes to training and deploying models locally. These <strong><em>drawbacks</em></strong> include:</p>
<ol>
<li><p>Sagemaker does not fully support Docker container local development environments. In other words, using the jupyter&#x2F;tensorflow-notebook image to develop Sagemaker sometimes generates minor issues. I will discuss this in more detail below.</p>
</li>
<li><p>Over-engineering. Honestly, although I am a supporter of Occam’s Razor and prefer solving practical problems with the simplest code, setting up Sagemaker on a local server can be somewhat over-engineered in terms of infrastructure.</p>
</li>
</ol>
<p>In summary, for long-term team development, it is necessary to spend time setting up Sagemaker locally in the short term.</p>
<h2 id="How-to-decide-whether-to-set-up-Sagemaker-on-a-local-server"><a href="#How-to-decide-whether-to-set-up-Sagemaker-on-a-local-server" class="headerlink" title="How to decide whether to set up Sagemaker on a local server?"></a>How to decide whether to set up Sagemaker on a local server?</h2><p>I referred to the method in the AWS official documentation to quickly let you know whether Sagemaker should be set up on a local server or not.<br><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers.html">https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers.html</a></p>
<p><img src="/images/why-choose-sagemaker-despite-having-a-local-server-with-rtx3080/workflow.png" alt="How to decide whether to set up Sagemaker on a local server?"></p>
<p><u>1. Do you use multiple deep learning frameworks?</u><br><strong>No</strong> -&gt; Use AWS cloud-based Sagemaker service. Maintain code simplicity and consistency.<br><strong>Yes</strong> -&gt; Go to question 2.</p>
<p><u>2. Is it team development?</u><br><strong>No</strong> -&gt; Use AWS cloud-based Sagemaker service. Automatically upload datasets and manage data versions.<br><strong>Yes</strong> -&gt; Go to question 3.</p>
<p><u>3. Is it long-term development?</u><br><strong>No</strong> -&gt; Use a local server. Save costs for long-term usage. However, AWS cloud-based services may not be necessary. It is recommended to use a local server with a graphics card.<br><strong>Yes</strong> -&gt; Go to question 4.</p>
<p><u>4. Is it deploying applications in the cloud?</u><br><strong>No</strong> -&gt; Use a local server.<br><strong>Yes</strong> -&gt; Set up Sagemaker on a local server. Efficiently utilize both the local server and AWS cloud-based services.</p>
<h2 id="Local-Server-Architecture"><a href="#Local-Server-Architecture" class="headerlink" title="Local Server Architecture"></a>Local Server Architecture</h2><p><img src="/images/why-choose-sagemaker-despite-having-a-local-server-with-rtx3080/local-server-architecture.png" alt="Local Server Architecture"></p>
<ol>
<li><p>Nvidia 11.5 driver. RTX3080 is required for both training and deploying models.</p>
</li>
<li><p>Nvidia-container-toolkit, connecting Docker images with Nvidia 11.5 driver.</p>
</li>
<li><p>Docker development container environment, jupyter&#x2F;tensorflow-notebook. Use Sagemaker to develop TensorFlow deep learning models.</p>
</li>
<li><p>Sagemaker training image. Sagemaker uses pre-built images to train models, automatically selecting suitable images for Nvidia, Python, and TensorFlow. Since I use TensorFlow, I use 763104351884.dkr.ecr.us-east-1.amazonaws.com&#x2F;tensorflow-training:2.11-gpu-py39.</p>
</li>
<li><p>Sagemaker server image. Sagemaker uses pre-built images to deploy models. This server image utilizes TensorFlow-serving (<a target="_blank" rel="noopener" href="https://github.com/tensorflow/serving">https://github.com/tensorflow/serving</a>) and Sagemaker’s inference for model deployment. Since I use TensorFlow, I use 763104351884.dkr.ecr.us-east-1.amazonaws.com&#x2F;tensorflow-inference:2.11-gpu.</p>
</li>
<li><p>S3 bucket. Used to centrally manage datasets and model versions.</p>
</li>
</ol>
<h2 id="Useful-Tips"><a href="#Useful-Tips" class="headerlink" title="Useful Tips"></a>Useful Tips</h2><p>Although these tips are very basic, in fast iteration cycles and team development, simple and practical tips can make development smoother and more efficient.</p>
<p><strong>Clear naming</strong><br>As the project develops over time, the number of dataset and model versions increases. Therefore, clear file naming conventions help maintain development efficiency.</p>
<p><strong>1. Prefix</strong></p>
<p><u>{Project Name}-{Model Type}-{Solution}</u></p>
<p>Whether it’s a dataset, model, or any temporary .csv file, it is best to have clear names to avoid forgetting the source and purpose of those files. Here are some examples of naming conventions I use.</p>
<blockquote>
<p>{futurePredict}-{lstm}-{t5}<br>{futurePredict}-{train}-{hloc}<br>{futurePredict}-{valid}-{hloc}</p>
</blockquote>
<p><strong>2. Suffix</strong></p>
<p><u>{Version Number}-{Timestamp}</u></p>
<p>After each model training, there are often new ideas. For example, when optimizing a LSTM model used for stock trading strategies by adding new momentum indicators, I would add this optimization approach to the suffix.</p>
<blockquote>
<p>{volSignal}-{20240106_130400}</p>
</blockquote>
<p>If there are no specific updates, generally, I use numbers to represent the current version.</p>
<blockquote>
<p>{a.1}-{20240106_130400}</p>
</blockquote>
<p><strong>3. Clear project structure</strong></p>
<p><u>.&#x2F;data&#x2F;input</u></p>
<p>Datasets inputted into the model.</p>
<p><u>.&#x2F;data&#x2F;output</u></p>
<p>Model outputs.</p>
<p><u>.&#x2F;data&#x2F;tmp</u></p>
<p>All temporary files. In fast iteration cycles, it is common to lose temporary files, leading to a loss of data source traceability. Therefore, temporary files also need to be well managed.</p>
<p><u>.&#x2F;model</u></p>
<p>Location for storing models. Generally, Sagemaker automatically manages datasets and models, but it is still recommended to store them locally for convenient team development.</p>
<p><u>.&#x2F;src</u></p>
<p>Supporting libraries, such as Sagemaker’s inference.py, and common toolkits for model training.</p>
<h2 id="Practical-Experience-Why-Sagemaker-Does-Not-Fully-Support-Local-Docker-Container-Development"><a href="#Practical-Experience-Why-Sagemaker-Does-Not-Fully-Support-Local-Docker-Container-Development" class="headerlink" title="Practical Experience: Why Sagemaker Does Not Fully Support Local Docker Container Development"></a>Practical Experience: Why Sagemaker Does Not Fully Support Local Docker Container Development</h2><p>The support of Sagemaker for local development is not very favorable. Below are two local development issues that I have encountered. Although I have found similar issues raised on Github, there is still no satisfactory solution available at present.</p>
<p><strong>1. Issue with local container Tensorflow-Jupyter development environment</strong></p>
<p>When training models, Sagemaker displays an error regarding <code>the docker container (No /opt/ml/input/config/resourceconfig.json)</code>.</p>
<p>The main reason is that after executing <code>estimator.fit(...)</code>, Sagemaker’s Training image reads temporary files in the <code>/tmp</code> path. However, Sagemaker does not consider the local container Tensorflow-Jupyter. As a result, these temporary files in <code>/tmp</code> are only available in the local container Tensorflow-Jupyter, causing errors when the Training image of Sagemaker tries to read them.</p>
<p>Here is the solution I provided:<br><a target="_blank" rel="noopener" href="https://github.com/aws/sagemaker-pytorch-training-toolkit/issues/106#issuecomment-1862233669">https://github.com/aws/sagemaker-pytorch-training-toolkit/issues/106#issuecomment-1862233669</a></p>
<p><img src="/images/why-choose-sagemaker-despite-having-a-local-server-with-rtx3080/github-001.png" alt="solution"></p>
<p><strong><em>Solution</em></strong>: When launching the local container Tensorflow-Jupyter, add the <code>&quot;-v /tmp:/tmp&quot;</code> command to link the local container’s <code>/tmp </code>with the local <code>/tmp</code>, which solves this problem.</p>
<p>Here is the code I used to launch the local container:<br><code>sudo docker run --privileged --name jupyter.sagemaker.001 --gpus all -e GRANT_SUDO=yes --user root --network host -it -v /home/jovyan/work:/home/jovyan/work -v /sagemaker:/sagemaker -v /var/run/docker.sock:/var/run/docker.sock -v /tmp:/tmp -v /sagemaker:/sagemaker sagemaker/local:0.2 &gt;&gt; /home/jovyan/work/log/sagemaker_local_date +\%Y\%m\%d_\%H\%M\%S.log 2</code></p>
<p><strong>2. Issue with Sagemaker’s local server image</strong><br>Sagemaker’s local server image defaults to using the inference method for deployment, so there is no inference.py in the server image. Therefore, <code>model.fit(...) </code>followed by <code>model.deploy(...)</code> results in errors.</p>
<p>The error messages are not clear either. Sometimes, it displays <code>&quot;/ping&quot;</code> error, and other times, <code>&quot;No such file or directory: &#39;inference.py&#39;&quot;</code> error.</p>
<p>Here is the solution I provided:<br><a target="_blank" rel="noopener" href="https://github.com/aws/sagemaker-python-sdk/issues/4007#issuecomment-1878176052">https://github.com/aws/sagemaker-python-sdk/issues/4007#issuecomment-1878176052</a></p>
<p><img src="/images/why-choose-sagemaker-despite-having-a-local-server-with-rtx3080/github-002.png" alt="solution"></p>
<p><strong><em>Solution</em></strong>: Save the model after <code>model.deploy(...)</code>. Then, use <code>sagemaker.tensorflow.TensorFlowModel(...)</code> to reload the model and reference <code>./src/inference.py</code>.</p>
<p>Although the inference method is a more convoluted way to locally deploy models, it is useful for adding middleware business logic on the server side and is a very valuable local deployment approach.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>I know that Sagemaker’s cloud service offers many amazing services, such as preprocessing data, batch training, Sagemaker-TensorBoard, and more. For developers who need to quickly prototype, these magical services are perfect for them.</p>
<p>Although setting up Sagemaker architecture on a local server may be more complex, Sagemaker provides standardized structure, automated processes, integrated unified interfaces, and pre-built resources. In the long run, I recommend setting up Sagemaker on a local server.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Kenny Chen"
      src="/images/icon/ic_avatar_001.jpg">
  <p class="site-author-name" itemprop="name">Kenny Chen</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">4</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://linkedin.com/in/kenny-chen-a560a8241" title="Linkin → https:&#x2F;&#x2F;linkedin.com&#x2F;in&#x2F;kenny-chen-a560a8241" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>Linkin</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://dev.to/kennc" title="https:&#x2F;&#x2F;dev.to&#x2F;kennc" rel="noopener" target="_blank">Dev.to</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.cnblogs.com/chenkuang" title="https:&#x2F;&#x2F;www.cnblogs.com&#x2F;chenkuang" rel="noopener" target="_blank">Cnblogs</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kenny Chen</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
